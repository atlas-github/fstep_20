{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "fstep_20_stats_students.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "6bANaR7ESl-F",
        "HNMpYbP0SpWO"
      ],
      "toc_visible": true,
      "authorship_tag": "ABX9TyMi8reRSTse9wRMFU61tvRz",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/atlas-github/fstep_20/blob/main/fstep_20_stats_students.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WvA9U2-DaLBO"
      },
      "source": [
        "#Linear regression, correlation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7rs49YnxawAx"
      },
      "source": [
        "The term regression is used when you try to **find the relationship between variables**.\n",
        "\n",
        "In Machine Learning, and in statistical modeling, that relationship is **used to predict the outcome of future events**.\n",
        "\n",
        "Linear regression uses the relationship between the data-points to draw a straight line through all them.\n",
        "\n",
        "This line can be used to predict future values.\n",
        "\n",
        "Source: [Machine Learning - Linear Regression](https://www.w3schools.com/python/python_ml_linear_regression.asp)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XL22TRrzaRtY"
      },
      "source": [
        "#Start by drawing a scatter plot\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "x = [5, 7, 8, 7, 2, 17, 2, 9, 4, 11, 12, 9, 6]\n",
        "y = [99, 86, 87, 88, 111, 86, 103, 87, 94, 78, 77, 85, 86]\n",
        "\n",
        "plt.scatter(x, y)\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HmO0dQ3bbgNm"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "from scipy import stats\n",
        "\n",
        "x = [5, 7, 8, 7, 2, 17, 2, 9, 4, 11, 12, 9, 6]\n",
        "y = [99, 86, 87, 88, 111, 86, 103, 87, 94, 78, 77, 85, 86]\n",
        "\n",
        "#Executes a method to return important key values of Linear Regression from https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.linregress.html\n",
        "\n",
        "### INSERT CODE HERE ###\n",
        "\n",
        "#Create a function that uses the slope and intercept values to return a new value. This new value represents where on the y-axis the corresponding x value will be placed\n",
        "def myfunc(x):\n",
        "  return slope * x + intercept\n",
        "\n",
        "#Run each value of the x array through the function. This will result in a new array with new values for the y-axis\n",
        "mymodel = list(map(myfunc, x))\n",
        "\n",
        "#Draw the original scatter plot\n",
        "plt.scatter(x, y)\n",
        "\n",
        "#Draw the line of linear regression\n",
        "plt.plot(x, mymodel)\n",
        "\n",
        "#Display the diagram\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z8mLwrMAcIKk"
      },
      "source": [
        "#Get the slope\n",
        "\n",
        "### INSERT CODE HERE ###"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6JSPvou_cK-l"
      },
      "source": [
        "#Get the intercept\n",
        "\n",
        "### INSERT CODE HERE ###"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gI8dOrDkcPow"
      },
      "source": [
        "#Get the r\n",
        "\n",
        "### INSERT CODE HERE ###"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PyvfvADuf7kp"
      },
      "source": [
        "It is common for different developers to get the same results using an alternate method. \n",
        "\n",
        "scikit-learn's [Linear Regression](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LinearRegression.html#sklearn.linear_model.LinearRegression) also has functions to calculate `slope`, `intercept`, and `r`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Uccx5PL6cgwx"
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.linear_model import LinearRegression\n",
        "\n",
        "X = np.array(x).reshape(-1, 1)\n",
        "Y = np.array(y).reshape(-1, 1)\n",
        "\n",
        "reg = LinearRegression().fit(X, Y)\n",
        "reg"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "alb0JBsZh0JX"
      },
      "source": [
        "##Exercise (Linear regression and correlation)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xOdhoOzXho2C"
      },
      "source": [
        "Refer to scikit-learn's [Linear Regression](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LinearRegression.html#sklearn.linear_model.LinearRegression), see if you can identify the functions needed to get the `slope`, `intercept`, and `r`. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TYCh7s-Aev4b"
      },
      "source": [
        "### INSERT CODE HERE ###"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_m3Rm0XJfSf7"
      },
      "source": [
        "### INSERT CODE HERE ###"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4EzLHkEcfcfy"
      },
      "source": [
        "### INSERT CODE HERE ###"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OZvn3CjxaSLs"
      },
      "source": [
        "#Logistic regression"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eO746Wex57Vv"
      },
      "source": [
        "Logistic regression is a **classification algorithm, used where the response variable is categorical**. \n",
        "\n",
        "Logistic regression is **used to find a relationship between features and probability of a particular outcome**. \n",
        "\n",
        "E.g. Predicting if a student passes or fails in an exam when the number of hours spent studying is given as a feature, the response variable has two values, pass and fail. \n",
        "\n",
        "More details [here](https://medium.com/data-science-group-iitr/logistic-regression-simplified-9b4efe801389).\n",
        "\n",
        "Source: [Understanding Logistic Regression in Python](https://www.datacamp.com/community/tutorials/understanding-logistic-regression-python)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0O0JED1QaTyt"
      },
      "source": [
        "!wget http://nrvis.com/data/mldata/pima-indians-diabetes.csv"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Jljm0qx661vn"
      },
      "source": [
        "This dataset is originally from the National Institute of Diabetes and Digestive and Kidney Diseases. The objective of the dataset is to diagnostically **predict whether or not a patient has diabetes**, based on certain diagnostic measurements included in the dataset. Several constraints were placed on the selection of these instances from a larger database. In particular, all patients here are females at least 21 years old of Pima Indian heritage.\n",
        "\n",
        "The datasets **consists of several medical predictor variables and one target variable, `label`**. \n",
        "\n",
        "Predictor variables includes the number of pregnancies the patient has had, their BMI, insulin level, age, and so on.\n",
        "\n",
        "Source: [Pima Indians Diabetes Database](https://www.kaggle.com/uciml/pima-indians-diabetes-database)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QEFO2UNXkecN"
      },
      "source": [
        "##Load data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EdR2gBJGkgIW"
      },
      "source": [
        "Start by loading the required Pima Indian Diabetes dataset using the pandas' read CSV function."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_izydUwm5Q--"
      },
      "source": [
        "import pandas as pd\n",
        "col_names = ['pregnant', 'glucose', 'bp', 'skin', 'insulin', 'bmi', 'pedigree', 'age', 'label']\n",
        "pima = pd.read_csv(\"pima-indians-diabetes.csv\", header=None, names=col_names)\n",
        "pima"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i2UBMD9qkncb"
      },
      "source": [
        "##Select feature"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7QVhXrQT7_3B"
      },
      "source": [
        "Here, you need to divide the given columns into two types of variables dependent (or target variable) and independent variable (or feature variables)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iHhzcHoq7xF4"
      },
      "source": [
        "#split dataset in features and target variable\n",
        "feature_cols = ['pregnant', 'insulin', 'bmi', 'age','glucose','bp', 'pedigree']\n",
        "X = pima[feature_cols] # Features\n",
        "y = pima.label # Target variable"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p6bJDLwCkqv1"
      },
      "source": [
        "##Split data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5AfSdM1F8LqX"
      },
      "source": [
        "To understand model performance, dividing the dataset into a training set and a test set is a good strategy.\n",
        "\n",
        "Let's split dataset by using function train_test_split(). You need to pass 3 parameters features, target, and test_set size. Additionally, you can use random_state to select records randomly."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qKbI4keN73op"
      },
      "source": [
        "# split X and y into training and testing sets\n",
        "from sklearn.model_selection import train_test_split\n",
        "X_train,X_test,y_train,y_test=train_test_split(X,y,test_size=0.25,random_state=0)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ogPh5uJ58R6p"
      },
      "source": [
        "Here, the Dataset is broken into two parts in a ratio of 75:25. It means 75% data will be used for model training and 25% for model testing."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "raWdhjcokxwX"
      },
      "source": [
        "##Develop model and predict"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ohmy25hykx9g"
      },
      "source": [
        "Import the Logistic Regression module and create a Logistic Regression classifier object using LogisticRegression() function.\n",
        "\n",
        "Then, fit your model on the train set using fit() and perform prediction on the test set using predict()."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s5DTVl0W74ut"
      },
      "source": [
        "# import the class\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "\n",
        "# instantiate the model (using the default parameters)\n",
        "logreg = LogisticRegression(solver = \"liblinear\")\n",
        "\n",
        "# fit the model with data\n",
        "logreg.fit(X_train,y_train)\n",
        "\n",
        "#\n",
        "y_pred=logreg.predict(X_test)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pZuFjB15k7IK"
      },
      "source": [
        "##Evaluate model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6kt28RBoSOSu"
      },
      "source": [
        "A confusion matrix is a table that is used to evaluate the performance of a classification model. You can also visualize the performance of an algorithm. The fundamental of a confusion matrix is the number of correct and incorrect predictions are summed up class-wise."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7i0cozScSR0Y"
      },
      "source": [
        "# import the metrics class\n",
        "from sklearn import metrics\n",
        "cnf_matrix = metrics.confusion_matrix(y_test, y_pred)\n",
        "cnf_matrix"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tLEPbLWqSTLE"
      },
      "source": [
        "Here, you can see the confusion matrix in the form of the array object. The dimension of this matrix is 2*2 because this model is binary classification. You have two classes 0 and 1. Diagonal values represent accurate predictions, while non-diagonal elements are inaccurate predictions. In the output, 119 and 36 are actual predictions, and 26 and 11 are incorrect predictions."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z5nriMhBlAUE"
      },
      "source": [
        "##Visualize confusion matrix"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oQ2iK_oMlAZk"
      },
      "source": [
        "Let's visualize the results of the model in the form of a confusion matrix using matplotlib and seaborn. Here, you will visualize the confusion matrix using Heatmap."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GlT-EqHvSU__"
      },
      "source": [
        "# import required modules\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "%matplotlib inline"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-gmipBFkSbIc"
      },
      "source": [
        "class_names=[0,1] # name  of classes\n",
        "fig, ax = plt.subplots()\n",
        "tick_marks = np.arange(len(class_names))\n",
        "plt.xticks(tick_marks, class_names)\n",
        "plt.yticks(tick_marks, class_names)\n",
        "# create heatmap\n",
        "sns.heatmap(pd.DataFrame(cnf_matrix), annot=True, cmap=\"YlGnBu\" ,fmt='g')\n",
        "ax.xaxis.set_label_position(\"top\")\n",
        "plt.tight_layout()\n",
        "plt.title('Confusion matrix', y=1.1)\n",
        "plt.ylabel('Actual label')\n",
        "plt.xlabel('Predicted label')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "55LgNJxglJFz"
      },
      "source": [
        "##Evaluation metrics"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qXeXYeZpSfXw"
      },
      "source": [
        "print(\"Accuracy:\",metrics.accuracy_score(y_test, y_pred))\n",
        "print(\"Precision:\",metrics.precision_score(y_test, y_pred))\n",
        "print(\"Recall:\",metrics.recall_score(y_test, y_pred))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V_iiiJBSlfuT"
      },
      "source": [
        "A classification rate of 80% is considered as good accuracy.\n",
        "\n",
        "Precision: Precision is about being precise, i.e., how accurate your model is. In other words, you can say, when a model makes a prediction, how often it is correct. In your prediction case, when your Logistic Regression model predicted patients are going to suffer from diabetes, that patients have 76% of the time.\n",
        "\n",
        "Recall: If there are patients who have diabetes in the test set and your Logistic Regression model can identify it 58% of the time."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6bANaR7ESl-F"
      },
      "source": [
        "##Advantages"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BTPtiIbMSniy"
      },
      "source": [
        "Because of its efficient and straightforward nature, doesn't require high computation power, easy to implement, easily interpretable, used widely by data analyst and scientist. Also, it doesn't require scaling of features. Logistic regression provides a probability score for observations."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HNMpYbP0SpWO"
      },
      "source": [
        "##Disadvantages"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G5f4gKHrSrwv"
      },
      "source": [
        "Logistic regression is not able to handle a large number of categorical features/variables. It is vulnerable to overfitting. Also, can't solve the non-linear problem with the logistic regression that is why it requires a transformation of non-linear features. Logistic regression will not perform well with independent variables that are not correlated to the target variable and are very similar or correlated to each other."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p805ipThnMF5"
      },
      "source": [
        "##Use `statsmodel.api` to get the coefficients "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ApDS52k-yGzO"
      },
      "source": [
        "import statsmodels.api as sm\n",
        "from statsmodels.tools import add_constant\n",
        "\n",
        "#x = add_constant(X)\n",
        "logit_model=sm.Logit(y, X)\n",
        "result=logit_model.fit()\n",
        "print(result.summary())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 102
        },
        "id": "RXAdZQ4G1Z56",
        "outputId": "aecf1d3f-f6ab-488d-9b07-c65b076279d3"
      },
      "source": [
        "from IPython.display import Image\n",
        "Image(\"https://www.saedsayad.com/images/LogReg_eq.png\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<IPython.core.display.Image object>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cUx-agfW2Q1O"
      },
      "source": [
        "Test the prediction function using `predict_proba`"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "93tP2KP9zLTe"
      },
      "source": [
        "X_test.head(1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f1E4_owG270f"
      },
      "source": [
        "*   pregnant: Number of times pregnant (0 to 17)\n",
        "*   insulin: 2-hour serum insulin (muU/ml) (0 to 846)\n",
        "*   bmi: Body mass index (0 to 67.1)\n",
        "*   age: Age in years (21 to 81)\n",
        "*   glucose: Plasma glucose concentration a 2 hours in an oral glucose tolerance test (0 to 199)\n",
        "*   bp: Diastolic blood pressure (mm Hg) (0 to 122)\n",
        "*   pedigree: Diabetes pedigree function (0.08 to 2.42)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1xvrs585tcpt"
      },
      "source": [
        "logreg.predict_proba(np.array(X_test.iloc[0, :]).reshape(1, -1))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n7S4hmY8zOQ7"
      },
      "source": [
        "logreg.predict_proba(np.array([1, 0, 42.9, 22, 199, 76, 1.394]).reshape(1, -1))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ANfd3s2flutb"
      },
      "source": [
        "##Exercise (Logistic regression)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "94zH8d_2VbSc"
      },
      "source": [
        "!wget https://archive.ics.uci.edu/ml/machine-learning-databases/adult/adult.data"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AdY6QGOFbPPG",
        "outputId": "0aec6d89-a455-4363-ea03-0439a6055d19"
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "#load dataset\n",
        "income = pd.read_csv(\"adult.data\", header = None, sep = \", \", names = [\"age\", \"workclass\", \"fnlwgt\", \"education\", \"educational-num\", \n",
        "                                                           \"marital\", \"occupation\", \"relationship\", \"race\", \"gender\", \n",
        "                                                           \"capital gain\", \"capital loss\", \"hours per week\", \"country\", \"income\"])\n",
        "\n",
        "#code will replace the special character to nan and then drop the columns \n",
        "income['country'] = income['country'].replace('?',np.nan)\n",
        "income['workclass'] = income['workclass'].replace('?',np.nan)\n",
        "income['occupation'] = income['occupation'].replace('?',np.nan)\n",
        "\n",
        "#dropping the NaN rows now \n",
        "income.dropna(how='any',inplace=True)\n",
        "\n",
        "#dropping based on uniquness of data from the dataset \n",
        "income.drop(['educational-num','age', 'hours per week', 'fnlwgt', 'capital gain','capital loss', 'country'], axis=1, inplace=True)\n",
        "\n",
        "#get rid of all leading white spaces\n",
        "income.applymap(str.strip)\n",
        "\n",
        "#mapping the data into numerical data using map function\n",
        "income['income'] = income['income'].map({'<=50K': 0, '>50K': 1}).astype(int)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:7: ParserWarning: Falling back to the 'python' engine because the 'c' engine does not support regex separators (separators > 1 char and different from '\\s+' are interpreted as regex); you can avoid this warning by specifying engine='python'.\n",
            "  import sys\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nz9WEPlbjCT0"
      },
      "source": [
        "#gender\n",
        "income['gender'] = income['gender'].map({'Male': 0, 'Female': 1}).astype(int)\n",
        "#race\n",
        "income['race'] = income['race'].map({'Black': 0, 'Asian-Pac-Islander': 1, 'Other': 2, 'White': 3, 'Amer-Indian-Eskimo': 4}).astype(int)\n",
        "#marital\n",
        "income['marital'] = income['marital'].map({'Married-spouse-absent': 0, 'Widowed': 1, 'Married-civ-spouse': 2, 'Separated': 3, 'Divorced': 4,'Never-married': 5, 'Married-AF-spouse': 6}).astype(int)\n",
        "#workclass\n",
        "income['workclass'] = income['workclass'].map({'Self-emp-inc': 0, 'State-gov': 1,'Federal-gov': 2, 'Without-pay': 3, 'Local-gov': 4,'Private': 5, 'Self-emp-not-inc': 6}).astype(int)\n",
        "#education\n",
        "income['education'] = income['education'].map({'Some-college': 0, 'Preschool': 1, '5th-6th': 2, 'HS-grad': 3, 'Masters': 4, '12th': 5, '7th-8th': 6, 'Prof-school': 7,'1st-4th': 8, 'Assoc-acdm': 9, 'Doctorate': 10, '11th': 11,'Bachelors': 12, '10th': 13,'Assoc-voc': 14,'9th': 15}).astype(int)\n",
        "#occupation\n",
        "income['occupation'] = income['occupation'].map({ 'Farming-fishing': 1, 'Tech-support': 2, 'Adm-clerical': 3, 'Handlers-cleaners': 4, \n",
        " 'Prof-specialty': 5,'Machine-op-inspct': 6, 'Exec-managerial': 7,'Priv-house-serv': 8,'Craft-repair': 9,'Sales': 10, 'Transport-moving': 11, 'Armed-Forces': 12, 'Other-service': 13,'Protective-serv':14}).astype(int)\n",
        "#relationship\n",
        "income['relationship'] = income['relationship'].map({'Not-in-family': 0, 'Wife': 1, 'Other-relative': 2, 'Unmarried': 3,'Husband': 4,'Own-child': 5}).astype(int)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2Rnhbf5Mlyjl"
      },
      "source": [
        "Use the data below to build a logistic regression model to predict income levels. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d71ZJBg2jCWV"
      },
      "source": [
        "income"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C02Y2NdJjCdN"
      },
      "source": [
        "#split dataset in features and target variable\n",
        "\n",
        "### INSERT CODE HERE ###\n",
        "\n",
        "X = income[feature_cols] # Features\n",
        "y = income.income # Target variable"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "umUXO7N6lKLl"
      },
      "source": [
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "#Initialize the linear regression model\n",
        "\n",
        "### INSERT CODE HERE ###\n",
        "\n",
        "#Split the data into 77% training and 33% testing data\n",
        "#NOTE: We have to split the dependent variables (x) and the target or independent variable (y)\n",
        "\n",
        "### INSERT CODE HERE ###"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5vCwlR7BlP1o"
      },
      "source": [
        "#Train our model with the training data\n",
        "\n",
        "### INSERT CODE HERE ###\n",
        "\n",
        "#print our price predictions on our test data\n",
        "\n",
        "### INSERT CODE HERE ###"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s8kN99ahlnr2"
      },
      "source": [
        "#feeding the predict function with our test values in the format [['relationship','education','race','occupation','gender','marital','workclass']]\n",
        "\n",
        "### INSERT CODE HERE ###"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_8D9TaEDls9i"
      },
      "source": [
        "from sklearn import metrics\n",
        "\n",
        "#printing the accuracy values \n",
        "\n",
        "### INSERT CODE HERE ###"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WUSpXOP3aWo4"
      },
      "source": [
        "#Hypothesis Testing"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mUB8QB6CteAP"
      },
      "source": [
        "One of the most famous libraries for hypothesis testing is [scipy](https://docs.scipy.org/doc/scipy/reference/stats.html)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LLXJ3IZQtrdT"
      },
      "source": [
        "##Shapiro-Wilk test for normality"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nbP0VxmVuF_K"
      },
      "source": [
        "The Shapiro-Wilk test tests the null hypothesis that the data was drawn from a normal distribution. [Documentation](https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.shapiro.html#scipy.stats.shapiro)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FD5lWo7DZ3df"
      },
      "source": [
        "# Example of the Shapiro-Wilk Normality Test\n",
        "from scipy.stats import shapiro\n",
        "data = [0.873, 2.817, 0.121, -0.945, -0.055, -1.436, 0.360, -1.478, -1.637, -1.869]\n",
        "\n",
        "### INSERT CODE HERE ###\n",
        "\n",
        "print('stat=%.3f, p=%.3f' % (stat, p))\n",
        "if p > 0.05:\n",
        "\tprint('Probably Gaussian')\n",
        "else:\n",
        "\tprint('Probably not Gaussian')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GAeNesrKuN3b"
      },
      "source": [
        "##Pearson's Correlation Coefficient"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MdIJVD1wumC7"
      },
      "source": [
        "Calculates a Pearson correlation coefficient and the p-value for testing non-correlation. [Documentation](https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.pearsonr.html#scipy.stats.pearsonr)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8MA-XttGujf4"
      },
      "source": [
        "# Example of the Pearson's Correlation test\n",
        "from scipy.stats import pearsonr\n",
        "data1 = [0.873, 2.817, 0.121, -0.945, -0.055, -1.436, 0.360, -1.478, -1.637, -1.869]\n",
        "data2 = [0.353, 3.517, 0.125, -7.545, -0.555, -1.536, 3.350, -1.578, -3.537, -1.579]\n",
        "\n",
        "### INSERT CODE HERE ###\n",
        "\n",
        "print('stat=%.3f, p=%.3f' % (stat, p))\n",
        "if p > 0.05:\n",
        "\tprint('Probably independent')\n",
        "else:\n",
        "\tprint('Probably dependent')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O-6S1tNFaeh_"
      },
      "source": [
        "##Chi-square test"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QvtODWnRu6G1"
      },
      "source": [
        "Tests whether two categorical variables are related or independent.\n",
        "\n",
        "Assumptions\n",
        "\n",
        "*   Observations used in the calculation of the contingency table are independent.\n",
        "*   25 or more examples in each cell of the contingency table.\n",
        "\n",
        "Interpretation\n",
        "\n",
        "*   H0: the two samples are independent.\n",
        "*   H1: there is a dependency between the samples.\n",
        "\n",
        "[Documentation](https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.chi2_contingency.html#scipy.stats.chi2_contingency)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8VSrXW7jaguq"
      },
      "source": [
        "# Example of the Chi-Squared Test\n",
        "from scipy.stats import chi2_contingency\n",
        "table = [[10, 20, 30],[6,  9,  17]]\n",
        "\n",
        "### INSERT CODE HERE ###\n",
        "\n",
        "print('stat=%.3f, p=%.3f' % (stat, p))\n",
        "if p > 0.05:\n",
        "\tprint('Probably independent')\n",
        "else:\n",
        "\tprint('Probably dependent')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jXg_W_n2v5Bf"
      },
      "source": [
        "##Paired t-test"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "32otfsd1v7bU"
      },
      "source": [
        "Tests whether the means of two paired samples are significantly different.\n",
        "\n",
        "Assumptions\n",
        "\n",
        "*   Observations in each sample are independent and identically distributed.\n",
        "*   Observations in each sample are normally distributed.\n",
        "*   Observations in each sample have the same variance.\n",
        "*   Observations across each sample are paired.\n",
        "\n",
        "Interpretation\n",
        "\n",
        "*   H0: the means of the samples are equal.\n",
        "*   H1: the means of the samples are unequal."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P_f28oxiqE5j"
      },
      "source": [
        "# Example of the Paired t-test\n",
        "from scipy.stats import ttest_rel\n",
        "data1 = [0.873, 2.817, 0.121, -0.945, -0.055, -1.436, 0.360, -1.478, -1.637, -1.869]\n",
        "data2 = [1.142, -0.432, -0.938, -0.729, -0.846, -0.157, 0.500, 1.183, -1.075, -0.169]\n",
        "\n",
        "### INSERT CODE HERE ###\n",
        "\n",
        "print('stat=%.3f, p=%.3f' % (stat, p))\n",
        "if p > 0.05:\n",
        "\tprint('Probably the same distribution')\n",
        "else:\n",
        "\tprint('Probably different distributions')"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}